{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c0ac3-6eaa-460b-b2db-89af2e033739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from strands import Agent, tool\n",
    "from strands.models.anthropic import AnthropicModel\n",
    "import httpx\n",
    "from typing import Dict, Any, List, Optional\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration - set your API base URL here\n",
    "BASE_URL = \"http://localhost:8000\"  # Change this to your deployed URL as needed\n",
    "\n",
    "# ============= STATISTICAL ANALYSIS FUNCTIONS =============\n",
    "\n",
    "def calculate_statistical_measures(data: List[Dict], field: str) -> Dict[str, float]:\n",
    "    \"\"\"Calculate comprehensive statistical measures for a numeric field.\"\"\"\n",
    "    try:\n",
    "        values = [float(item.get('properties', {}).get(field, 0)) for item in data if item.get('properties', {}).get(field) is not None]\n",
    "        if not values:\n",
    "            return {}\n",
    "        \n",
    "        values = np.array(values)\n",
    "        return {\n",
    "            'mean': float(np.mean(values)),\n",
    "            'std': float(np.std(values)),\n",
    "            'median': float(np.median(values)),\n",
    "            'q25': float(np.percentile(values, 25)),\n",
    "            'q75': float(np.percentile(values, 75)),\n",
    "            'min': float(np.min(values)),\n",
    "            'max': float(np.max(values)),\n",
    "            'cv': float(np.std(values) / np.mean(values)) if np.mean(values) != 0 else 0,\n",
    "            'skewness': float(stats.skew(values)),\n",
    "            'kurtosis': float(stats.kurtosis(values))\n",
    "        }\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def detect_outliers_zscore(data: List[Dict], field: str, threshold: float = 3.0) -> Dict[str, Any]:\n",
    "    \"\"\"Detect outliers using Z-score method.\"\"\"\n",
    "    try:\n",
    "        values = [float(item.get('properties', {}).get(field, 0)) for item in data if item.get('properties', {}).get(field) is not None]\n",
    "        if len(values) < 3:\n",
    "            return {'outlier_count': 0, 'outlier_percentage': 0, 'z_threshold': threshold}\n",
    "        \n",
    "        z_scores = np.abs(stats.zscore(values))\n",
    "        outliers = z_scores > threshold\n",
    "        return {\n",
    "            'outlier_count': int(np.sum(outliers)),\n",
    "            'outlier_percentage': float(np.sum(outliers) / len(values) * 100),\n",
    "            'z_threshold': threshold,\n",
    "            'max_z_score': float(np.max(z_scores))\n",
    "        }\n",
    "    except Exception:\n",
    "        return {'outlier_count': 0, 'outlier_percentage': 0, 'z_threshold': threshold}\n",
    "\n",
    "def perform_trend_analysis(data: List[Dict], time_field: str, value_field: str) -> Dict[str, Any]:\n",
    "    \"\"\"Perform linear trend analysis on time series data.\"\"\"\n",
    "    try:\n",
    "        # Extract time and value pairs\n",
    "        time_values = []\n",
    "        values = []\n",
    "        for item in data:\n",
    "            props = item.get('properties', {})\n",
    "            if props.get(time_field) and props.get(value_field):\n",
    "                time_values.append(float(props[time_field]))\n",
    "                values.append(float(props[value_field]))\n",
    "        \n",
    "        if len(time_values) < 3:\n",
    "            return {}\n",
    "        \n",
    "        # Perform linear regression\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(time_values, values)\n",
    "        \n",
    "        return {\n",
    "            'slope': float(slope),\n",
    "            'r_squared': float(r_value ** 2),\n",
    "            'p_value': float(p_value),\n",
    "            'correlation': float(r_value),\n",
    "            'trend_strength': 'strong' if abs(r_value) > 0.7 else 'moderate' if abs(r_value) > 0.4 else 'weak',\n",
    "            'trend_direction': 'increasing' if slope > 0 else 'decreasing',\n",
    "            'significance': 'significant' if p_value < 0.05 else 'not_significant'\n",
    "        }\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def analyze_categorical_distribution(data: List[Dict], field: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze distribution of categorical data.\"\"\"\n",
    "    try:\n",
    "        categories = [item.get('properties', {}).get(field) for item in data if item.get('properties', {}).get(field)]\n",
    "        if not categories:\n",
    "            return {}\n",
    "        \n",
    "        from collections import Counter\n",
    "        counts = Counter(categories)\n",
    "        total = len(categories)\n",
    "        \n",
    "        # Calculate entropy for distribution evenness\n",
    "        probabilities = [count/total for count in counts.values()]\n",
    "        entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "        max_entropy = np.log2(len(counts))\n",
    "        normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'unique_categories': len(counts),\n",
    "            'most_common': counts.most_common(1)[0] if counts else None,\n",
    "            'entropy': float(entropy),\n",
    "            'normalized_entropy': float(normalized_entropy),\n",
    "            'distribution_evenness': 'even' if normalized_entropy > 0.8 else 'moderate' if normalized_entropy > 0.5 else 'skewed'\n",
    "        }\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def test_correlation(data: List[Dict], field1: str, field2: str) -> Dict[str, Any]:\n",
    "    \"\"\"Test correlation between two numeric fields.\"\"\"\n",
    "    try:\n",
    "        values1 = []\n",
    "        values2 = []\n",
    "        for item in data:\n",
    "            props = item.get('properties', {})\n",
    "            if props.get(field1) and props.get(field2):\n",
    "                values1.append(float(props[field1]))\n",
    "                values2.append(float(props[field2]))\n",
    "        \n",
    "        if len(values1) < 3:\n",
    "            return {}\n",
    "        \n",
    "        # Pearson correlation\n",
    "        pearson_r, pearson_p = stats.pearsonr(values1, values2)\n",
    "        \n",
    "        # Spearman correlation (rank-based)\n",
    "        spearman_r, spearman_p = stats.spearmanr(values1, values2)\n",
    "        \n",
    "        return {\n",
    "            'pearson_correlation': float(pearson_r),\n",
    "            'pearson_p_value': float(pearson_p),\n",
    "            'spearman_correlation': float(spearman_r),\n",
    "            'spearman_p_value': float(spearman_p),\n",
    "            'correlation_strength': 'strong' if abs(pearson_r) > 0.7 else 'moderate' if abs(pearson_r) > 0.4 else 'weak',\n",
    "            'pearson_significant': pearson_p < 0.05,\n",
    "            'spearman_significant': spearman_p < 0.05\n",
    "        }\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "# ============= API-CALLING TOOLS =============\n",
    "\n",
    "@tool\n",
    "async def get_recent_financial_data(limit: int = 16) -> dict:\n",
    "    \"\"\"\n",
    "    Get the most recent financial data rows for analysis.\n",
    "    \n",
    "    Args:\n",
    "        limit (int): Number of recent rows to fetch (default: 100, max: 1000)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Financial data response with events\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure limit doesn't exceed 1000\n",
    "        limit = min(limit, 100)\n",
    "        \n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.get(\n",
    "                f\"{BASE_URL}/api/agent-query\",\n",
    "                params={\"limit\": limit},\n",
    "                timeout=30.0\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"data\": data.get(\"events\", data) if isinstance(data, dict) else data,\n",
    "                \"count\": len(data.get(\"events\", data) if isinstance(data, dict) else data),\n",
    "                \"message\": f\"Retrieved {len(data.get('events', data) if isinstance(data, dict) else data)} financial records\"\n",
    "            }\n",
    "    \n",
    "    except httpx.HTTPStatusError as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data\": [],\n",
    "            \"count\": 0,\n",
    "            \"message\": f\"HTTP {e.response.status_code} error: {str(e)}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data\": [],\n",
    "            \"count\": 0,\n",
    "            \"message\": f\"Error retrieving financial data: {str(e)}\"\n",
    "        }\n",
    "\n",
    "@tool\n",
    "async def get_current_graphs() -> dict:\n",
    "    \"\"\"\n",
    "    Get all currently displayed graphs from the dashboard.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Response containing list of current graphs with their configurations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.get(\n",
    "                f\"{BASE_URL}/api/graphs/\",\n",
    "                timeout=30.0\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            graphs = response.json()\n",
    "\n",
    "            print(\"GOT GRAPHS\")\n",
    "            print(graphs)\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"graphs\": graphs,\n",
    "                \"count\": len(graphs),\n",
    "                \"message\": f\"Retrieved {len(graphs)} graphs from dashboard\"\n",
    "            }\n",
    "    \n",
    "    except httpx.HTTPStatusError as e:\n",
    "        print(f'ERROR: {e}')\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"graphs\": [],\n",
    "            \"count\": 0,\n",
    "            \"message\": f\"HTTP {e.response.status_code} error: {str(e)}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f'ERROR: {e}')\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"graphs\": [],\n",
    "            \"count\": 0,\n",
    "            \"message\": f\"Error retrieving graphs: {str(e)}\"\n",
    "        }\n",
    "\n",
    "@tool\n",
    "async def remove_graph(graph_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Remove a graph by its ID from the dashboard.\n",
    "    \n",
    "    Args:\n",
    "        graph_id (str): ID of the graph to remove\n",
    "    \n",
    "    Returns:\n",
    "        dict: Removal result with confirmation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.delete(\n",
    "                f\"{BASE_URL}/api/graphs/{graph_id}\",\n",
    "                timeout=30.0\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"removed_id\": graph_id,\n",
    "                \"message\": result.get(\"message\", f\"Successfully removed graph {graph_id}\")\n",
    "            }\n",
    "    \n",
    "    except httpx.HTTPStatusError as e:\n",
    "        if e.response.status_code == 404:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"removed_id\": None,\n",
    "                \"message\": f\"Graph {graph_id} not found\"\n",
    "            }\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"removed_id\": None,\n",
    "            \"message\": f\"HTTP {e.response.status_code} error: {str(e)}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"removed_id\": None,\n",
    "            \"message\": f\"Error removing graph: {str(e)}\"\n",
    "        }\n",
    "\n",
    "@tool\n",
    "async def add_new_graph(type: str, title: str, sql_query: str, justification: str, extra: Optional[Dict] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Add a new graph to the financial dashboard with statistical justification.\n",
    "    \n",
    "    Args:\n",
    "        type (str): Type of graph (bar, line, pie, area, scatter, scatter3d)\n",
    "        title (str): Title for the graph\n",
    "        sql_query (str): SQL query to fetch data for the graph\n",
    "        justification (str): Statistical justification including numerical test results\n",
    "        extra (Optional[Dict]): Additional parameters like axis labels\n",
    "    \n",
    "    Returns:\n",
    "        dict: Addition result with new graph details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare the graph configuration\n",
    "        graph_config = {\n",
    "            \"type\": type,\n",
    "            \"title\": title,\n",
    "            \"sql_query\": sql_query,\n",
    "            \"justification\": justification\n",
    "        }\n",
    "        \n",
    "        if extra:\n",
    "            graph_config[\"extra\"] = extra\n",
    "        \n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.post(\n",
    "                f\"{BASE_URL}/api/graphs/\",\n",
    "                json=graph_config,\n",
    "                timeout=30.0\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            new_graph = response.json()\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"new_graph_id\": new_graph.get(\"id\"),\n",
    "                \"graph\": new_graph,\n",
    "                \"message\": f\"Successfully added graph: {title}\"\n",
    "            }\n",
    "    \n",
    "    except httpx.HTTPStatusError as e:\n",
    "        print('ERROR!!!!')\n",
    "        print(e)\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"new_graph_id\": None,\n",
    "            \"graph\": None,\n",
    "            \"message\": f\"HTTP {e.response.status_code} error: {str(e)}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"new_graph_id\": None,\n",
    "            \"graph\": None,\n",
    "            \"message\": f\"Error adding graph: {str(e)}\"\n",
    "        }\n",
    "\n",
    "@tool\n",
    "async def update_graph(graph_id: str, type: str, sql_query: str, justification: str, title: Optional[str] = None, extra: Optional[Dict] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Update an existing graph's configuration with statistical justification.\n",
    "    \n",
    "    Args:\n",
    "        graph_id (str): ID of graph to update\n",
    "        type (str): New graph type (bar, line, pie, area, scatter, scatter3d)\n",
    "        sql_query (str): New SQL query for the graph\n",
    "        justification (str): Statistical justification including numerical test results\n",
    "        title (Optional[str]): New title for the graph\n",
    "        extra (Optional[Dict]): Additional parameters like axis labels\n",
    "    \n",
    "    Returns:\n",
    "        dict: Update result with changes made\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare update data\n",
    "        update_data = {\n",
    "            \"type\": type,\n",
    "            \"sql_query\": sql_query,\n",
    "            \"justification\": justification\n",
    "        }\n",
    "        \n",
    "        if title:\n",
    "            update_data[\"title\"] = title\n",
    "        \n",
    "        if extra:\n",
    "            update_data[\"extra\"] = extra\n",
    "        \n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.put(\n",
    "                f\"{BASE_URL}/api/graphs/{graph_id}\",\n",
    "                json=update_data,\n",
    "                timeout=30.0\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            updated_graph = response.json()\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"updated_graph_id\": graph_id,\n",
    "                \"graph\": updated_graph,\n",
    "                \"changes\": update_data,\n",
    "                \"message\": f\"Successfully updated graph {graph_id}\"\n",
    "            }\n",
    "    \n",
    "    except httpx.HTTPStatusError as e:\n",
    "        if e.response.status_code == 404:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"updated_graph_id\": None,\n",
    "                \"message\": f\"Graph {graph_id} not found\"\n",
    "            }\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"updated_graph_id\": None,\n",
    "            \"message\": f\"HTTP {e.response.status_code} error: {str(e)}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"updated_graph_id\": None,\n",
    "            \"message\": f\"Error updating graph: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# ============= MODEL & AGENT CONFIGURATION =============\n",
    "\n",
    "def create_agent(api_key: str):\n",
    "    \"\"\"\n",
    "    Create the financial analysis agent with API tools.\n",
    "    \n",
    "    Args:\n",
    "        api_key (str): Anthropic API key\n",
    "    \n",
    "    Returns:\n",
    "        Agent: Configured agent instance\n",
    "    \"\"\"\n",
    "    \n",
    "    model = AnthropicModel(\n",
    "        client_args={\n",
    "            \"api_key\": api_key,\n",
    "        },\n",
    "        max_tokens=4096,\n",
    "        model_id=\"claude-sonnet-4-20250514\",  # Updated to use available model\n",
    "        params={\n",
    "            \"temperature\": 0.3,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Only include the API-calling tools\n",
    "    agent = Agent(\n",
    "        model=model, \n",
    "        tools=[\n",
    "            get_recent_financial_data,\n",
    "            get_current_graphs,\n",
    "            remove_graph,\n",
    "            add_new_graph,\n",
    "            update_graph\n",
    "        ],\n",
    "        system_prompt=get_enhanced_financial_prompt(4),\n",
    "    )\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# ============= ENHANCED PROMPT WITH STATISTICAL ANALYSIS =============\n",
    "\n",
    "def get_enhanced_financial_prompt(max_graphs: int) -> str:\n",
    "    \"\"\"\n",
    "    Returns the comprehensive financial analysis prompt with embedded statistical analysis logic.\n",
    "    \n",
    "    Args:\n",
    "        max_graphs (int): Maximum number of graphs to maintain\n",
    "        \n",
    "    Returns:\n",
    "        str: Detailed financial analysis prompt with statistical requirements\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "You are a sophisticated Financial Data Analysis Agent with advanced statistical capabilities. Execute comprehensive graph management with rigorous statistical justification for every decision.\n",
    "\n",
    "## STATISTICAL ANALYSIS REQUIREMENTS\n",
    "\n",
    "### When analyzing financial data, you MUST perform these statistical tests:\n",
    "\n",
    "1. **Descriptive Statistics**: Calculate mean, median, std dev, quartiles, skewness, kurtosis\n",
    "2. **Outlier Detection**: Use Z-score analysis (threshold=3.0) to identify anomalous transactions\n",
    "3. **Trend Analysis**: Perform linear regression for time series (R², p-values, slope significance)\n",
    "4. **Distribution Analysis**: Calculate entropy and distribution evenness for categories\n",
    "5. **Correlation Testing**: Pearson and Spearman correlation with significance tests\n",
    "6. **Variance Analysis**: Coefficient of variation to measure relative variability\n",
    "\n",
    "### Justification Format Requirements:\n",
    "\n",
    "For every `add_new_graph()` and `update_graph()` call, the `justification` parameter MUST include:\n",
    "\n",
    "```\n",
    "Statistical Analysis Results:\n",
    "- Sample size: [N] observations\n",
    "- Key metric statistics: mean=[X], std=[Y], CV=[Z]%\n",
    "- Outlier detection: [N] outliers ([X]%) detected using Z-score>3.0\n",
    "- Trend analysis: R²=[X], p-value=[Y], slope=[Z] ([significant/not significant])\n",
    "- Distribution: entropy=[X], evenness=[even/moderate/skewed]\n",
    "- Correlation: r=[X], p=[Y] ([strong/moderate/weak] and [significant/not significant])\n",
    "- Business impact: [High/Medium/Low] - [specific reason]\n",
    "```\n",
    "\n",
    "## GRAPH TYPES & SQL FORMAT REQUIREMENTS\n",
    "\n",
    "### Table\n",
    "Always use the table 'events'. The SQL query should always use FROM events.\n",
    "\n",
    "### Queries\n",
    "For accessing properties in events, use the format properties->>'property_name' for text and (properties->>'property_name')::NUMERIC for numeric values.\n",
    "\n",
    "### Available Graph Types:\n",
    "1. **bar** - Category vs Value comparison\n",
    "   - SQL Format: (category TEXT, value NUMERIC)\n",
    "   - Use for: Department expenses, regional comparisons, categorical metrics\n",
    "   - Extra params: y_axis_label\n",
    "\n",
    "2. **line** - Time series trends  \n",
    "   - SQL Format: (time INT8, value NUMERIC)\n",
    "   - Use for: Revenue over time, customer growth, trend analysis\n",
    "   - Extra params: x_axis_label, y_axis_label\n",
    "\n",
    "3. **pie** - Composition/Share of whole\n",
    "   - SQL Format: (slice TEXT, value NUMERIC)  \n",
    "   - Use for: Revenue breakdown, market share, portfolio composition\n",
    "\n",
    "4. **area** - Time series with filled area\n",
    "   - SQL Format: (time INT8, value NUMERIC)\n",
    "   - Use for: Cumulative trends, volume over time\n",
    "   - Extra params: x_axis_label, y_axis_label\n",
    "\n",
    "5. **scatter** - Correlation/Distribution analysis\n",
    "   - SQL Format: (x_value NUMERIC, y_value NUMERIC, size NUMERIC NULL, label TEXT NULL)\n",
    "   - Use for: Risk vs return, cost vs volume correlations\n",
    "   - Extra params: x_axis_label, y_axis_label\n",
    "   \n",
    "## ENHANCED WORKFLOW EXECUTION WITH STATISTICAL ANALYSIS\n",
    "\n",
    "### 1. CURRENT GRAPH ANALYSIS & STATISTICAL VALIDATION\n",
    "\n",
    "**Step 1a: Get Current State**\n",
    "- Use `get_current_graphs()` to retrieve all existing graphs\n",
    "- Analyze each graph's statistical validity and current performance\n",
    "\n",
    "**Step 1b: Statistical Re-evaluation of Existing Graphs**\n",
    "For each existing graph, mentally perform:\n",
    "- Freshness analysis: Is the underlying pattern still statistically significant?\n",
    "- Efficiency analysis: Can the SQL query capture stronger statistical relationships?\n",
    "- Type optimization: Does the current graph type best represent the statistical pattern?\n",
    "\n",
    "If statistical improvements are identified, use `update_graph()` with full statistical justification.\n",
    "\n",
    "### 2. COMPREHENSIVE STATISTICAL DATA ANALYSIS\n",
    "\n",
    "**Step 2a: Data Acquisition & Preparation**\n",
    "- Use `get_recent_financial_data(limit=16)` to fetch latest financial records\n",
    "- Prepare data for statistical analysis\n",
    "\n",
    "**Step 2b: Multi-Dimensional Statistical Discovery**\n",
    "Systematically analyze for these patterns:\n",
    "\n",
    "**ANOMALY DETECTION:**\n",
    "- Z-score analysis for transaction amounts (identify values >3σ from mean)\n",
    "- Frequency anomalies in categorical data\n",
    "- Time-based clustering analysis\n",
    "\n",
    "**TREND & CORRELATION ANALYSIS:**\n",
    "- Time series regression analysis (R², significance tests)\n",
    "- Cross-variable correlation matrices\n",
    "- Leading indicator identification\n",
    "\n",
    "**DISTRIBUTION PATTERNS:**\n",
    "- Categorical distribution entropy calculations\n",
    "- Variance decomposition analysis\n",
    "- Seasonal/cyclical pattern detection\n",
    "\n",
    "### 3. STATISTICAL-BASED GRAPH PRIORITIZATION\n",
    "\n",
    "**Step 3a: Statistical Significance Ranking**\n",
    "Rank discovered patterns by:\n",
    "- **Statistical Significance**: p-values < 0.05 get priority\n",
    "- **Effect Size**: R² > 0.5 for correlations, Cohen's d > 0.8 for differences\n",
    "- **Business Impact**: Revenue/risk implications\n",
    "- **Novelty Score**: Unique insights not currently displayed\n",
    "\n",
    "**Step 3b: Rigorous Graph Configuration**\n",
    "For each high-priority pattern:\n",
    "- Select graph type based on statistical characteristics\n",
    "- Design SQL queries optimized for statistical clarity\n",
    "- Calculate comprehensive justification metrics\n",
    "- Ensure statistical assumptions are met\n",
    "\n",
    "### 4. STRATEGIC PORTFOLIO MANAGEMENT WITH STATISTICS\n",
    "\n",
    "**Step 4a: Portfolio Statistical Assessment**\n",
    "- Evaluate current graphs' statistical strength and relevance\n",
    "- Identify redundant or statistically weak visualizations\n",
    "- Assess coverage across fraud detection, performance metrics, and market analysis\n",
    "\n",
    "**Step 4b: Evidence-Based Graph Management**\n",
    "- Remove graphs with weakest statistical foundation if at capacity\n",
    "- Add new graphs with strongest statistical justification\n",
    "- Always include complete statistical justification in API calls\n",
    "\n",
    "### 5. DETAILED STATISTICAL REPORTING\n",
    "\n",
    "Document your complete analysis including:\n",
    "- **Statistical Summary**: All test results and significance levels\n",
    "- **Decision Matrix**: How statistical evidence influenced graph choices\n",
    "- **Methodology**: Which statistical tests were applied and why\n",
    "- **Confidence Levels**: Uncertainty quantification for each insight\n",
    "- **Actionable Recommendations**: Business decisions supported by statistical evidence\n",
    "\n",
    "## CRITICAL STATISTICAL REQUIREMENTS\n",
    "\n",
    "1. **Mandatory Justification**: Every `add_new_graph()` and `update_graph()` call MUST include detailed statistical justification with numerical results\n",
    "2. **Significance Testing**: Only create graphs for statistically significant patterns (p < 0.05 preferred)\n",
    "3. **Effect Size**: Prioritize patterns with meaningful effect sizes (R² > 0.3, Cohen's d > 0.5)\n",
    "4. **Sample Size Validation**: Acknowledge limitations when N < 30\n",
    "5. **Multiple Testing**: Consider Bonferroni correction when testing multiple hypotheses\n",
    "6. **Statistical Assumptions**: Verify normality, independence, and homoscedasticity as appropriate\n",
    "\n",
    "Execute this statistically-rigorous workflow to maintain an evidence-based financial intelligence dashboard with maximum analytical value.\n",
    "\"\"\"\n",
    "\n",
    "# ============= MAIN EXECUTION FUNCTIONS =============\n",
    "\n",
    "def run_graph_management_agent(\n",
    "    api_key: str,\n",
    "    interval_seconds: int = 20, \n",
    "    max_iterations: Optional[int] = 1, \n",
    "    max_graphs: int = 4\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the enhanced financial graph management agent continuously.\n",
    "    \n",
    "    Args:\n",
    "        api_key (str): Anthropic API key\n",
    "        interval_seconds (int): How often to run the agent (default: 20 seconds)\n",
    "        max_iterations (Optional[int]): Maximum number of iterations (None for infinite)\n",
    "        max_graphs (int): Maximum number of graphs to maintain (default: 6)\n",
    "    \"\"\"\n",
    "    print(f\"🚀 Starting Enhanced Statistical Financial Analysis Agent...\")\n",
    "    print(f\"📊 Update interval: {interval_seconds} seconds\")\n",
    "    print(f\"📈 Max graphs: {max_graphs}\")\n",
    "    print(f\"🔄 Max iterations: {'Unlimited' if max_iterations is None else max_iterations}\")\n",
    "    print(f\"🌐 API Base URL: {BASE_URL}\")\n",
    "    print(f\"📊 Statistical Analysis: Advanced statistical tests enabled\")\n",
    "    print(f\"⏰ Start time: {datetime.now()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create agent with API key\n",
    "    agent = create_agent(api_key)\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    try:\n",
    "        while max_iterations is None or iteration < max_iterations:\n",
    "            iteration += 1\n",
    "            \n",
    "            print(f\"\\n{'='*15} 📊 STATISTICAL ANALYSIS CYCLE {iteration} {'='*15}\")\n",
    "            print(f\"🕐 Starting comprehensive statistical analysis at {datetime.now()}\")\n",
    "            \n",
    "            try:\n",
    "                result = agent(\"Start the statistical workflow with comprehensive analysis and justification for every graph decision\")\n",
    "                print(f\"✅ [CYCLE {iteration}] Statistical financial analysis completed successfully\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ [CYCLE {iteration}] Agent error: {e}\")\n",
    "            \n",
    "            if max_iterations is None or iteration < max_iterations:\n",
    "                print(f\"⏳ [CYCLE {iteration}] Waiting {interval_seconds} seconds until next analysis...\")\n",
    "                print(\"-\" * 80)\n",
    "                time.sleep(interval_seconds)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n\\n🛑 Received interrupt signal. Stopping after {iteration} iterations.\")\n",
    "        print(f\"🏁 End time: {datetime.now()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n💥 Unexpected error in analysis loop: {e}\")\n",
    "        print(f\"🏁 Stopped after {iteration} iterations at {datetime.now()}\")\n",
    "\n",
    "# ============= MAIN ENTRY POINT =============\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get API key from environment or prompt user\n",
    "    api_key = os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
    "    \n",
    "    if not api_key:\n",
    "        print(\"⚠️  No ANTHROPIC_API_KEY found in environment variables.\")\n",
    "        api_key = input(\"Please enter your Anthropic API key: \").strip()\n",
    "    \n",
    "    if not api_key:\n",
    "        print(\"❌ API key is required to run the agent.\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"🎯 Running Enhanced Statistical Financial Analysis Agent for 10 iterations...\")\n",
    "    run_graph_management_agent(\n",
    "        api_key=api_key,\n",
    "        interval_seconds=20, \n",
    "        max_iterations=10, \n",
    "        max_graphs=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db47a0d-1112-4045-8366-fa11ea1aef28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
